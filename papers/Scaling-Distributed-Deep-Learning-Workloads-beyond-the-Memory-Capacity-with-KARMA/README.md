# Scaling Distributed Deep Learning Workloads beyond the Memory Capacity with KARMA

Wahib, Mohamed, et al. "Scaling distributed deep learning workloads beyond the memory capacity with KARMA." SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020.

## What
Performance model based on the concurrency analysis of out-of-core training behavior

## Why
Big models do not fit into the GPU memory.

## How


## Notes