# WhippleTree

Source code link: https://www.tugraz.at/institutes/icg/research/team-schmalstieg/software-media/gpu-downloads/

## Learn more about this stuff

* For multiprogramming, check multi-instance gpu on A100.
* Check whether parent and child kernels need to communicate through global memory.

## Notes

Problems that they want to address:

* Multiprogramming capability: Simultaneous execution of multiple pipeline stages.
* Dynamic Parallelism: Dynamic work generation (a kernel spawns another kernel)
* Data Locality: Usage of shared memory
* Support for different kinds of parallelism: Work to be done in different pipeline stages may take advantage of different parallelism types.

## Persistent Threads

A single kernel keeps each SM continuously occupied. All threads spin in a loop and draw new work items from a software queue in each iteration. New work can dynamically be generated by simply pushing items into the queue.

* Contention from many threads can quickly make the work queue become a major bottleneck. 
* Multiple distributed queues, e. g., one per SM, can be used to mitigate contention. 
* To achieve load balancing in such a setup, work stealing or work donation can be used.

## Megakernels

Works around the limitation that only a single graphics pipeline configuration can be active at any time.

Puts all potentially needed functionality into one big kernel and branches to different subroutines based on input.

In a **persistent megakernel** all threads run in a loop, draw some work from a queue, and branch to the corresponding subroutine.

* The choice of the number of threads processing each element drawn from the queue has a defining influence on the properties of the resulting system. 
* Distributing items to individual threads can lead to severe thread divergence and prevents features like shared memory from being used.

## Time-sliced Kernels

Uses dedicated queues per kernel.

* TSK is only applicable to problems which can be efficiently separated into individual stages.

## Dynamic Parallelism

Allows launching new kernels directly from device code.

* The book keeping necessary to track the kernel launch hierarchy seems to be a performance limiting factor.
* Data communication between kernels has to go through slow global memory (not sure if this is still true).

## Programming Model of WhippleTree

There are three types of tasks:

* Level 0 Tasks: Tasks that must be executed by threads of the same warp.
* Level 1 Tasks: Tasks that threads are to be executed on the same SM.
* Level 2 Tasks: Single-threaded tasks that can run on any SM.

Building blocks:

* Work Items: Data elements to be processed.
* Procedures: Code that implements the processing of work items. Declares number of threads required to process a single work item.
* Tasks: Created by specifying a work item and the procedure the work item should be handed to. Whippletree takes care of scheduling available tasks for execution in a way that respects the constraints associated with the respective task type. During execution, a task can dynamically spawn new tasks.
* Programs: Set of procedures. Self-contained in the sense that any procedure in a program may only spawn tasks for procedures that are also part of the program.

Producedure for recursive split stage of Reyes rendering pipeline: 

```C++
struct Quad {
    Vertex v[4];
};

struct Split : public Procedure {
    typedef Quad WorkItem;
    static const int NumThreads = 2;
    static const int TaskType = WARP;
    static const int SharedMemory = 0;

    template<class Context>
    __device__ static void execute(int threadId, const Quad* quad, void* shared) {
        float3 mDist = quad.v[threadId+1] - quad.v[0];
        float approxS = length(mDist);
        float sizeX = Context::shfl(approxS,0);
        float sizeY = Context::shfl(approxS,1);
        Quad out;
        if(sizeX > sizeY) {
            float3 v01 = quad.v[1] - quad.v[0];
            float3 v23 = quad.v[3] - quad.v[2];
            out.v[0] = quad.v[0] + 0.5f*threadId*v01;
            out.v[1] = quad.v[0] + 0.5f*(threadId+1)*v01;
            out.v[2] = quad.v[2] + 0.5f*threadId*v23;
            out.v[3] = quad.v[2] + 0.5f*(threadId+1)*v23;
        } else { ... }
        
        if(out.size() > Splitthreshold)
            spawn<Split>(out);
        else
            spawn<DiceAndSample>(out);
    }
};
```

Host code to init and run a simple Reyes pipeline:

```C++
struct SimpleReyesProgram : public Program<Bound, Split, DiceAndSample, Shade> {};

typedef Whippletree<SimpleReyesProgram,GlobalPerProcedureQueuing> ReyesPipeline;

struct QuadInit {
    typedef Quad* InputData;
    
    __device__ static void init(int threadId, InputData quads) {
        spawn<Bound>(quads[threadId]);
    }
};

void initAndRunPipeline(Model& model) {
    ReyesPipeline reyes;
    reyes.init<QuadInit>(model.num(), model.quads());
    reyes.run();
}
```

## Implementation

* Blocks of threads (worker-blocks) are launched to exactly fill up all SMs. These worker-blocks execute a loop drawing tasks from work queues (one queue per procedure).
* Procedures are implemented as branches in the main loop. When new tasks are generated, they are inserted into the queues.
* Synchs all threads inside a block at the beginning of each loop iteration to have a sufficient number of idle threads available to start the execution of any procedure.

### Dynamic worker-blocks

If the queue holds:

* Level 0 tasks: Computes how many tasks fit in a warp and try to fill up all warps in the block.
* Level 1 tasks: Draws as many level-1 tasks from the queue as can be executed concurrently with the available number of threads.
* Level 2 tasks: Tries to draw one task for each thread.

### Queue Management

Two level hierarchy: 

* A small amount of shared memory is used to implement the top level of the hierarchy. Requests are always answered by the top level queues if possible. 
* To support load balancing among worker-blocks and SM units, the lower-level hierarchy implements queues in global memory.
* Global queues use a fixed size ring buffer with front and back pointers.
* After deciding the worker-block size, the remaining shared memory is reserved for local queuing.